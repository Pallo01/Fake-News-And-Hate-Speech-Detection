{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1854178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D,LSTM, Dropout\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cdb085",
   "metadata": {},
   "source": [
    "### Loading Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a38e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_fake = pd.read_csv(\"datasets/news/Fake.csv\")\n",
    "df_true = pd.read_csv(\"datasets/news/True.csv\")\n",
    "df_true2 =pd.read_csv(\"datasets/news/News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908247c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the labels as 0 and 1 to fake and real news datasets\n",
    "df_fake[\"label\"] = 0\n",
    "df_true[\"label\"] = 1\n",
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marge = pd.concat([df_fake, df_true, df_true2], axis =0 )\n",
    "df_marge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unwanted columns\n",
    "df = df_marge.drop([\"title\", \"subject\",\"date\"], axis = 1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7180d4bc",
   "metadata": {},
   "source": [
    "### Final Datasets is df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf24775",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row which have null value in column 'text'\n",
    "df = df.dropna(axis=0, subset=['text'])\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40800c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffling the dataframe \n",
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing index column which was generated after reshuffling the dataframe\n",
    "df.reset_index(inplace = True)\n",
    "df.drop([\"index\"], axis = 1, inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ceb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6db6cdb",
   "metadata": {},
   "source": [
    "#### Creating a function to convert the text in lowercase, remove the extra space, special chr., ulr and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(wordopt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ef83f1",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "##### Lemmatization is the process of reducing words to their base or root form, which can help to group together words with similar meanings and reduce the number of unique words in a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e29d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download necessary resources for tokenization and lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Define a function to lemmatize a single word\n",
    "def lemmatize_word(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "# Define a function to lemmatize a list of words\n",
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatize_word(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40814e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the lemmatization function to the 'text' column of the DataFrame\n",
    "df['text'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dependent and independent variable as x and y\n",
    "X = df[\"text\"]\n",
    "Y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2270380e",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ade9b2c0",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "##### It is the process of dividing a text into smaller units (each word will be an index in an array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# Converting text to sequence\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = 500\n",
    "# padding\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "# Exporting Tokenizer\n",
    "import joblib\n",
    "joblib.dump(tokenizer,\"models/fakeNews/tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fcc4415",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "#### CNNs are commonly used for text classification tasks such as fake news detection. They can learn to detect patterns and features in the text by using convolutional layers and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "CNN = Sequential()\n",
    "CNN.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=max_len))\n",
    "CNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "CNN.add(GlobalMaxPooling1D())\n",
    "CNN.add(Dense(units=64, activation='relu'))\n",
    "CNN.add(Dropout(rate=0.2))\n",
    "CNN.add(Dense(units=1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "CNN.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = CNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "CNN.save('models/fakeNews/CNN.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec0616b",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "#### RNNs are another popular choice for text classification tasks. They can process sequential data by using feedback loops, allowing them to capture the context and meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "RNN = Sequential()\n",
    "RNN.add(Embedding(5000, 128, input_length=X_train.shape[1]))\n",
    "RNN.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "RNN.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa773222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# RNN.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
    "RNN.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ffd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = RNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "RNN.save('models/fakeNews/RNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_testing(news):\n",
    "    new_article=news\n",
    "    new_article = wordopt(new_article)\n",
    "    new_article = lemmatize_text(new_article)\n",
    "    new_article = tokenizer.texts_to_sequences([new_article])\n",
    "    padded = pad_sequences(new_article, maxlen=X_train.shape[1])\n",
    "    pred_CNN = CNN.predict(new_article)\n",
    "    pred_RNN = RNN.predict(padded)\n",
    "    return print(\"\\n\\nCNN Prediction: {} \\nRNN Prediction: {}\".format(pred_CNN,pred_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb252b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = str(input())\n",
    "manual_testing(news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "601f2eab1f90ba50d03b799f76b2d604ffb284c6c9562cc1c1496b99197443b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
