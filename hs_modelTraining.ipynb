{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D,LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/HateSpeech/HateSpeech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was a better world when women dare not ques...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't work this hard so that those immigrant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  It should be legal for a husband to rape his wife      1\n",
       "1                            dalits are lowlife scum      1\n",
       "2                                dalits are lowlives      1\n",
       "3  It was a better world when women dare not ques...      1\n",
       "4  I don't work this hard so that those immigrant...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25362</th>\n",
       "      <td>our country requires a good scrub I will begin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23614</th>\n",
       "      <td>let\u0018s talk about the antithesis of emotional s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48493</th>\n",
       "      <td>Aye we all needa keep are heads up high who ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23330</th>\n",
       "      <td>No one uses 'niglets' because its fucking rude</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53168</th>\n",
       "      <td>Lmfao RT @Luvv_55st Eating pussy RT @DanaBlack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "25362  our country requires a good scrub I will begin...      1\n",
       "23614  let\u0018s talk about the antithesis of emotional s...      0\n",
       "48493  Aye we all needa keep are heads up high who ca...      1\n",
       "23330     No one uses 'niglets' because its fucking rude      0\n",
       "53168  Lmfao RT @Luvv_55st Eating pussy RT @DanaBlack...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly shuffling the dataframe \n",
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our country requires a good scrub I will begin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>let\u0018s talk about the antithesis of emotional s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aye we all needa keep are heads up high who ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No one uses 'niglets' because its fucking rude</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lmfao RT @Luvv_55st Eating pussy RT @DanaBlack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  our country requires a good scrub I will begin...      1\n",
       "1  let\u0018s talk about the antithesis of emotional s...      0\n",
       "2  Aye we all needa keep are heads up high who ca...      1\n",
       "3     No one uses 'niglets' because its fucking rude      0\n",
       "4  Lmfao RT @Luvv_55st Eating pussy RT @DanaBlack...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index\n",
    "df.reset_index(inplace = True)\n",
    "df.drop([\"index\"], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to convert the text in lowercase, remove the extra space, special chr., ulr and links.\n",
    "import re\n",
    "import string\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text\n",
    "# function call\n",
    "df['text']=df['text'].apply(wordopt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "##### Lemmatization is the process of reducing words to their base or root form, which can help to group together words with similar meanings and reduce the number of unique words in a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashisgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashisgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashisgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download necessary resources for tokenization and lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Define a function to lemmatize a list of words\n",
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    return ' '.join(lemmatized_words)\n",
    "# Apply the lemmatization function to the 'text' column of the DataFrame\n",
    "df['text'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dependent and independent variable as x and y\n",
    "X = df[\"text\"]\n",
    "Y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/hateSpeech/tokenizer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "max_len = 500 # Maximum length of input sequences\n",
    "vocab_size = len(tokenizer.word_index) + 1 # Size of the vocabulary\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "# Exporting Tokenizer\n",
    "import joblib\n",
    "joblib.dump(tokenizer,\"models/hateSpeech/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "#### CNNs are commonly used for text classification tasks such as fake news detection. They can learn to detect patterns and features in the text by using convolutional layers and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = Sequential()\n",
    "CNN.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))\n",
    "CNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "CNN.add(GlobalMaxPooling1D())\n",
    "CNN.add(Dense(units=64, activation='relu'))\n",
    "CNN.add(Dropout(rate=0.2))\n",
    "CNN.add(Dense(units=1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "818/818 [==============================] - 66s 80ms/step - loss: 0.4419 - accuracy: 0.7773 - val_loss: 0.3881 - val_accuracy: 0.8161\n",
      "Epoch 2/5\n",
      "818/818 [==============================] - 61s 75ms/step - loss: 0.3440 - accuracy: 0.8411 - val_loss: 0.3830 - val_accuracy: 0.8178\n",
      "Epoch 3/5\n",
      "818/818 [==============================] - 62s 76ms/step - loss: 0.2756 - accuracy: 0.8802 - val_loss: 0.4144 - val_accuracy: 0.8122\n",
      "Epoch 4/5\n",
      "818/818 [==============================] - 62s 75ms/step - loss: 0.2112 - accuracy: 0.9120 - val_loss: 0.4494 - val_accuracy: 0.8022\n",
      "Epoch 5/5\n",
      "818/818 [==============================] - 62s 76ms/step - loss: 0.1620 - accuracy: 0.9344 - val_loss: 0.5077 - val_accuracy: 0.8045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16fe32f4408>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "CNN.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 2s 4ms/step\n",
      "Accuracy: 80.45%\n",
      "Confusion Matrix:  [[3282 1191]\n",
      " [1366 7243]]\n"
     ]
    }
   ],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = CNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "CNN.save('models/hateSpeech/CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "#### RNNs are another popular choice for text classification tasks. They can process sequential data by using feedback loops, allowing them to capture the context and meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "RNN = Sequential()\n",
    "RNN.add(Embedding(5000, 128, input_length=max_len))\n",
    "RNN.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "RNN.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "818/818 [==============================] - 2271s 3s/step - loss: 0.6471 - accuracy: 0.6519 - val_loss: 0.6425 - val_accuracy: 0.6581\n",
      "Epoch 2/5\n",
      "818/818 [==============================] - 2432s 3s/step - loss: 0.6464 - accuracy: 0.6524 - val_loss: 0.6424 - val_accuracy: 0.6581\n",
      "Epoch 3/5\n",
      "818/818 [==============================] - 2445s 3s/step - loss: 0.6465 - accuracy: 0.6524 - val_loss: 0.6440 - val_accuracy: 0.6581\n",
      "Epoch 4/5\n",
      "818/818 [==============================] - 2841s 3s/step - loss: 0.6464 - accuracy: 0.6524 - val_loss: 0.6425 - val_accuracy: 0.6581\n",
      "Epoch 5/5\n",
      "818/818 [==============================] - 2753s 3s/step - loss: 0.6462 - accuracy: 0.6524 - val_loss: 0.6434 - val_accuracy: 0.6581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16fe33a4888>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "RNN.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 30s 72ms/step\n",
      "Accuracy: 65.81%\n",
      "Confusion Matrix:  [[   0 4473]\n",
      " [   0 8609]]\n"
     ]
    }
   ],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = RNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "RNN.save('models/hateSpeech/RNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_testing(speech):\n",
    "    speech = wordopt(speech)\n",
    "    speech = lemmatize_text(speech)\n",
    "    speech_seq = tokenizer.texts_to_sequences([speech])\n",
    "    speech_pad = pad_sequences(speech_seq, padding='post', maxlen=500)\n",
    "    pred_CNN = CNN.predict(speech_pad)\n",
    "    pred_RNN = RNN.predict(speech_pad)\n",
    "    return print(\"\\n\\nCNN Prediction: {} \\nRNN Prediction: {}\".format(pred_CNN,pred_RNN))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model With manual Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "\n",
      "\n",
      "CNN Prediction: [[0.12929419]] \n",
      "RNN Prediction: [[0.63576514]]\n"
     ]
    }
   ],
   "source": [
    "speech = str(input())\n",
    "manual_testing(speech)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47ce0aed635e00af2fbd4ed33da37445a4ef4710a85474e4eaacbf68ef84f273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
