{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D,LSTM, Dropout\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/HateSpeech/HateSpeech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffling the dataframe \n",
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df.reset_index(inplace = True)\n",
    "df.drop([\"index\"], axis = 1, inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to convert the text in lowercase, remove the extra space, special chr., ulr and links.\n",
    "import re\n",
    "import string\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text\n",
    "# function call\n",
    "df['text']=df['text'].apply(wordopt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "##### Lemmatization is the process of reducing words to their base or root form, which can help to group together words with similar meanings and reduce the number of unique words in a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download necessary resources for tokenization and lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Define a function to lemmatize a single word\n",
    "def lemmatize_word(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "# Define a function to lemmatize a list of words\n",
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatize_word(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "# Apply the lemmatization function to the 'text' column of the DataFrame\n",
    "df['text'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dependent and independent variable as x and y\n",
    "X = df[\"text\"]\n",
    "Y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "max_len = 500 # Maximum length of input sequences\n",
    "vocab_size = len(tokenizer.word_index) + 1 # Size of the vocabulary\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "# Exporting Tokenizer\n",
    "import joblib\n",
    "joblib.dump(tokenizer,\"models/hateSpeech/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "#### CNNs are commonly used for text classification tasks such as fake news detection. They can learn to detect patterns and features in the text by using convolutional layers and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = Sequential()\n",
    "CNN.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))\n",
    "CNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "CNN.add(GlobalMaxPooling1D())\n",
    "CNN.add(Dense(units=64, activation='relu'))\n",
    "CNN.add(Dropout(rate=0.2))\n",
    "CNN.add(Dense(units=1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "CNN.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = CNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "CNN.save('models/hateSpeech/CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "#### RNNs are another popular choice for text classification tasks. They can process sequential data by using feedback loops, allowing them to capture the context and meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "RNN = Sequential()\n",
    "RNN.add(Embedding(5000, 128, input_length=max_len))\n",
    "RNN.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "RNN.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "RNN.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = RNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "RNN.save('models/hateSpeech/RNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_testing(news):\n",
    "    new_article=news\n",
    "    new_article = wordopt(new_article)\n",
    "    new_article = lemmatize_text(new_article)\n",
    "    new_article = tokenizer.texts_to_sequences([new_article])\n",
    "    padded = pad_sequences(new_article, maxlen=500)\n",
    "    pred_CNN = CNN.predict(new_article)\n",
    "    pred_RNN = RNN.predict(padded)\n",
    "    return print(\"\\n\\nCNN Prediction: {} \\nRNN Prediction: {}\".format(pred_CNN,pred_RNN))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model With manual Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = str(input())\n",
    "manual_testing(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy and Confusion Matrix\n",
    "y_pred = RNN.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {round(acc_score*100,2)}%')\n",
    "print(\"Confusion Matrix: \", cm)\n",
    "# Save the model\n",
    "RNN.save('models/RNN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "601f2eab1f90ba50d03b799f76b2d604ffb284c6c9562cc1c1496b99197443b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
